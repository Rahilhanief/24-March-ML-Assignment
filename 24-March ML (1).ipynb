{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13422b5c-01e1-4679-aeb2-2b718778e787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFixed acidity. The predominant fixed acids in wine, such as tartaric, succinic, citric, and malic acids.\\nVolatile acidity. The high acetic acid present in wine, which causes an unpleasant vinegar taste.\\nCitric acid. A weak organic acid used to increase the freshness and flavor of wine.\\nResidual sugar. The amount of sugar left after fermentation.\\nChlorides. The amount of salt in wine. The lower chloride rate creates better quality wines.\\nFree sulfur dioxide. SO2 is used for preventing wine from oxidation and microbial spoilage.\\nTotal sulfur dioxide. The amount of free and bound forms of SO2.\\nDensity. Depends on the alcohol and sugar content. Better wines usually have lower densities.\\npH. Used to check the level of acidity or alkalinity of wine.\\nSulfates. An antibacterial and antioxidant agent added to wine.\\nAlcohol. The percentage of alcohol in wine. A higher concentration leads to better quality.\\nThe output variable is, therefore, the quality rating of wine that is based on sensory data and scores from 0 to 10.\\nAltogether, these features are crucial for getting the most accurate and reliable predictions from a machine learning model.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1 :\n",
    "\"\"\"\n",
    "Fixed acidity. The predominant fixed acids in wine, such as tartaric, succinic, citric, and malic acids.\n",
    "Volatile acidity. The high acetic acid present in wine, which causes an unpleasant vinegar taste.\n",
    "Citric acid. A weak organic acid used to increase the freshness and flavor of wine.\n",
    "Residual sugar. The amount of sugar left after fermentation.\n",
    "Chlorides. The amount of salt in wine. The lower chloride rate creates better quality wines.\n",
    "Free sulfur dioxide. SO2 is used for preventing wine from oxidation and microbial spoilage.\n",
    "Total sulfur dioxide. The amount of free and bound forms of SO2.\n",
    "Density. Depends on the alcohol and sugar content. Better wines usually have lower densities.\n",
    "pH. Used to check the level of acidity or alkalinity of wine.\n",
    "Sulfates. An antibacterial and antioxidant agent added to wine.\n",
    "Alcohol. The percentage of alcohol in wine. A higher concentration leads to better quality.\n",
    "The output variable is, therefore, the quality rating of wine that is based on sensory data and scores from 0 to 10.\n",
    "Altogether, these features are crucial for getting the most accurate and reliable predictions from a machine learning model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a205f81f-765c-4cb9-9ebd-613b2b7a2e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'By using the code \\ndf.isnull().sum() we can handle the missing values in any dataset.\\n\\nThese are some of the data imputation techniques that we will be discussing in-depth:\\n\\nNext or Previous Value \\nK Nearest Neighbors\\nMaximum or Minimum Value\\nMissing Value Prediction\\nMost Frequent Value\\nAverage or Linear Interpolation\\n(Rounded) Mean or Moving Average or Median Value\\nFixed Value\\nWe will be exploring each of these techniques in a detailed manner now.\\n\\n1. Next or Previous Value\\nFor time-series data or ordered data, there are specific imputation techniques.\\nThese techniques take into consideration the dataset\\'s sorted structure, \\nwherein nearby values are likely more comparable than far-off ones. \\nThe next or previous value inside the time series is typically substituted for the missing value as part of a common method for imputed incomplete data in the time series. This strategy is effective for both nominal and numerical values.\\n\\n2. K Nearest Neighbors\\nThe objective is to find the k nearest examples in the data where the value in the relevant feature is not absent and then substitute the value of the feature that occurs most frequently in the group.\\n\\n3. Maximum or Minimum Value\\nYou can use the minimum or maximum of the range as the replacement cost for missing values if you are aware that the data must fit within a specific range [minimum, maximum] and if you are aware from the process of data collection that the measurement instrument stops recording and the message saturates further than one of such boundaries. For instance, if a price cap has been reached in a financial exchange and the exchange procedure has indeed been halted, the missing price can be substituted with the exchange boundary\\'s minimum value.\\n\\n4. Missing Value Prediction\\nUsing a machine learning model to determine the final imputation value for characteristic x based on other features is another popular method for single imputation. The model is trained using the values in the remaining columns, and the rows in feature x without missing values are utilized as the training set. \\n\\nDepending on the type of feature, we can employ any regression or classification model in this situation. In resistance training, the algorithm is used to forecast the most likely value of each missing value in all samples.\\n\\nA basic imputation approach, such as the mean value, is used to temporarily impute all missing values when there is missing data in more than a feature field. Then, one column\\'s values are restored to missing. After training, the model is used to complete the missing variables. In this manner, an is trained for every feature that has a missing value up until a model can impute all of the missing values.\\n\\n5. Most Frequent Value\\nThe most frequent value in the column is used to replace the missing values in another popular technique that is effective for both nominal and numerical features.\\n\\n6. Average or Linear Interpolation\\nThe average or linear interpolation, which calculates between the previous and next accessible value and substitutes the missing value, is similar to the previous/next value imputation but only applicable to numerical data. Of course, as with other operations on ordered data, it is crucial to accurately sort the data in advance, for example, in the case of time series data, according to a timestamp.\\n\\n7. (Rounded) Mean or Moving Average or Median Value\\nMedian, Mean, or rounded mean are further popular imputation techniques for numerical features. The technique, in this instance, replaces the null values with mean, rounded mean, or median values determined for that feature across the whole dataset. It is advised to utilize the median rather than the mean when your dataset has a significant number of outliers.\\n\\n8. Fixed Value\\nFixed value imputation is a universal technique that replaces the null data with a fixed value and is applicable to all data types. You can impute the null values in a survey using \"not answered\" as an example of using fixed imputation on nominal features.\\n\\nSince we have explored single imputation, its importance, and its techniques, let us now learn about Multiple imputations.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 2 :\n",
    "\"\"\"By using the code \n",
    "df.isnull().sum() we can handle the missing values in any dataset.\n",
    "\n",
    "These are some of the data imputation techniques that we will be discussing in-depth:\n",
    "\n",
    "Next or Previous Value \n",
    "K Nearest Neighbors\n",
    "Maximum or Minimum Value\n",
    "Missing Value Prediction\n",
    "Most Frequent Value\n",
    "Average or Linear Interpolation\n",
    "(Rounded) Mean or Moving Average or Median Value\n",
    "Fixed Value\n",
    "We will be exploring each of these techniques in a detailed manner now.\n",
    "\n",
    "1. Next or Previous Value\n",
    "For time-series data or ordered data, there are specific imputation techniques.\n",
    "These techniques take into consideration the dataset's sorted structure, \n",
    "wherein nearby values are likely more comparable than far-off ones. \n",
    "The next or previous value inside the time series is typically substituted for the missing value as part of a common method for imputed incomplete data in the time series. This strategy is effective for both nominal and numerical values.\n",
    "\n",
    "2. K Nearest Neighbors\n",
    "The objective is to find the k nearest examples in the data where the value in the relevant feature is not absent and then substitute the value of the feature that occurs most frequently in the group.\n",
    "\n",
    "3. Maximum or Minimum Value\n",
    "You can use the minimum or maximum of the range as the replacement cost for missing values if you are aware that the data must fit within a specific range [minimum, maximum] and if you are aware from the process of data collection that the measurement instrument stops recording and the message saturates further than one of such boundaries. For instance, if a price cap has been reached in a financial exchange and the exchange procedure has indeed been halted, the missing price can be substituted with the exchange boundary's minimum value.\n",
    "\n",
    "4. Missing Value Prediction\n",
    "Using a machine learning model to determine the final imputation value for characteristic x based on other features is another popular method for single imputation. The model is trained using the values in the remaining columns, and the rows in feature x without missing values are utilized as the training set. \n",
    "\n",
    "Depending on the type of feature, we can employ any regression or classification model in this situation. In resistance training, the algorithm is used to forecast the most likely value of each missing value in all samples.\n",
    "\n",
    "A basic imputation approach, such as the mean value, is used to temporarily impute all missing values when there is missing data in more than a feature field. Then, one column's values are restored to missing. After training, the model is used to complete the missing variables. In this manner, an is trained for every feature that has a missing value up until a model can impute all of the missing values.\n",
    "\n",
    "5. Most Frequent Value\n",
    "The most frequent value in the column is used to replace the missing values in another popular technique that is effective for both nominal and numerical features.\n",
    "\n",
    "6. Average or Linear Interpolation\n",
    "The average or linear interpolation, which calculates between the previous and next accessible value and substitutes the missing value, is similar to the previous/next value imputation but only applicable to numerical data. Of course, as with other operations on ordered data, it is crucial to accurately sort the data in advance, for example, in the case of time series data, according to a timestamp.\n",
    "\n",
    "7. (Rounded) Mean or Moving Average or Median Value\n",
    "Median, Mean, or rounded mean are further popular imputation techniques for numerical features. The technique, in this instance, replaces the null values with mean, rounded mean, or median values determined for that feature across the whole dataset. It is advised to utilize the median rather than the mean when your dataset has a significant number of outliers.\n",
    "\n",
    "8. Fixed Value\n",
    "Fixed value imputation is a universal technique that replaces the null data with a fixed value and is applicable to all data types. You can impute the null values in a survey using \"not answered\" as an example of using fixed imputation on nominal features.\n",
    "\n",
    "Since we have explored single imputation, its importance, and its techniques, let us now learn about Multiple imputations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd66f5d3-898d-4f0e-945d-cca10016c84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- Standard Lunch help students perform well in exams\\n\\n- Standard lunch helps perform well in exams be it a male of female\\n- Students of group A and group B tends to perform poorly in exam.\\n- Students of group A and group B tends to perform poorly in exam irrespective of whether they are male or female\\n\\nData Checks to perform\\nCheck Missing values\\nCheck Duplicates\\nCheck data type\\nCheck the number of unique values of each column\\nCheck statistics of data set\\nCheck various categories present in the different categorical column\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 3 :\n",
    "\"\"\"\n",
    "- Standard Lunch help students perform well in exams\n",
    "\n",
    "- Standard lunch helps perform well in exams be it a male of female\n",
    "- Students of group A and group B tends to perform poorly in exam.\n",
    "- Students of group A and group B tends to perform poorly in exam irrespective of whether they are male or female\n",
    "\n",
    "Data Checks to perform\n",
    "Check Missing values\n",
    "Check Duplicates\n",
    "Check data type\n",
    "Check the number of unique values of each column\n",
    "Check statistics of data set\n",
    "Check various categories present in the different categorical column\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd89b14-dc1b-4132-b487-3656e329b8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFeature Engineering is the process of creating new features or transforming existing features to improve the \\nperformance of a machine-learning model.\\nIt involves selecting relevant information from raw data and transforming it into a format that can be easily\\nunderstood by a model. \\nThe goal is to improve model accuracy by providing more meaningful and relevant information.\\n\\nsuppose  we have created two new fetures average and total_score which will give us idea about the performance of\\nstudents\\n\\n\\ndf['total_score']=(df['math_score']+df['reading_score']+df['writing_score'])\\ndf['average']=df['total_score']/3\\ndf.head()\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "\"\"\"\n",
    "Feature Engineering is the process of creating new features or transforming existing features to improve the \n",
    "performance of a machine-learning model.\n",
    "It involves selecting relevant information from raw data and transforming it into a format that can be easily\n",
    "understood by a model. \n",
    "The goal is to improve model accuracy by providing more meaningful and relevant information.\n",
    "\n",
    "suppose  we have created two new fetures average and total_score which will give us idea about the performance of\n",
    "students\n",
    "\n",
    "\n",
    "df['total_score']=(df['math_score']+df['reading_score']+df['writing_score'])\n",
    "df['average']=df['total_score']/3\n",
    "df.head()\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394a0745-4a48-49bb-a15b-0d8cc86ab100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import pandas as pd\\ndf =pd.read_csv('winequality-red.csv')\\ndf.head()\\ndf.info()\\ndf.describe()\\ndf.isnull().sum()\\ndf.shape\\ndf.value_counts\\ndf[df.duplicated()]\\n\\nResidual sugar\\nTransformations could be applied to\\nthese features to improve normality are :\\nLogarithmic transformation\\n\\nSquare root transformation\\n\\nExponential transformation\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 5 :\n",
    "\"\"\"import pandas as pd\n",
    "df =pd.read_csv('winequality-red.csv')\n",
    "df.head()\n",
    "df.info()\n",
    "df.describe()\n",
    "df.isnull().sum()\n",
    "df.shape\n",
    "df.value_counts\n",
    "df[df.duplicated()]\n",
    "\n",
    "Residual sugar\n",
    "Transformations could be applied to\n",
    "these features to improve normality are :\n",
    "Logarithmic transformation\n",
    "\n",
    "Square root transformation\n",
    "\n",
    "Exponential transformation\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "532bf08d-3d4d-4212-9e02-b3764bd928fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPCA is a dimensionality reduction technique that has four main parts: feature covariance, eigendecomposition, \\nprincipal component transformation, and choosing components in terms of explained variance.\\n20 components.\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 6 :\n",
    "\"\"\"\n",
    "PCA is a dimensionality reduction technique that has four main parts: feature covariance, eigendecomposition, \n",
    "principal component transformation, and choosing components in terms of explained variance.\n",
    "20 components.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
